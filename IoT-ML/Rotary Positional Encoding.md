Continues [[The Transformer]]

![[Pasted image 20250421164606.png]]
# Relative Encoding
- Rather than adding a vector that encodes absolute position in the sequence to each token
- Create a function that encodes relative distance between tokens and add this vector to the key vector before multiplying by the Query matrix
# Rotary Position Embeddings: Inner Product

For class #IoT-ML